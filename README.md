# Titanic Production-Grade MLOps Pipeline

This project implements a complete, production-ready MLOps pipeline for the Titanic dataset using the following stack:

- **Git**: Code versioning.
- **DVC**: Data versioning and pipeline orchestration.
- **MLflow**: Experiment tracking and Model Registry.
- **GitHub Actions**: CI/CD automation.
- **Python / Scikit-Learn**: Machine learning logic.

## Project Structure

```text
.
├── .github/workflows/mlops.yml  # CI/CD pipeline
├── data/
│   ├── raw/                    # V1: Raw data
│   ├── interim/                # V2: Cleaned data
│   └── processed/              # V3: Engineered features
├── src/
│   ├── clean_data.py           # Data cleaning logic
│   ├── feature_engineering.py  # Feature engineering logic
│   ├── train.py                # Training script (MLflow)
│   ├── register_model.py       # Auto-model selection & registration
│   └── generate_report.py      # PDF report generator
├── dvc.yaml                    # DVC pipeline definition
├── dvc.lock                    # DVC pipeline lockfile
├── mlflow.db                   # MLflow sqlite backend
└── mlruns/                     # MLflow tracking data
```

## End-to-End Steps to Reproduce

### 1. Environment Setup

Install the required dependencies:

```bash
pip install -r requirements.txt
```

### 2. Dataset Versioning (DVC)

The data is versioned in three stages:

- **V1 (Raw)**: Tracked with `dvc add data/raw/train.csv`.
- **V2 (Cleaned)**: Generated by `src/clean_data.py`.
- **V3 (Engineered)**: Generated by `src/feature_engineering.py`.

### 3. Running the Pipeline

The entire pipeline is automated with DVC. To execute all stages from scratch:

```bash
dvc repro -f
```

This command runs:

1. `clean_data`: Handles nulls, encoding, and scaling.
2. `feature_engineering`: Creates `FamilySize`, `IsAlone`, and extracts `Title`.
3. `training`: Trains Logistic Regression and Random Forest, logging metrics and models to MLflow.
4. `registration`: Compares models in MLflow and promotes the best one (highest F1-score) to **Production** in the Model Registry.

### 4. MLflow Tracking

To view the experiment results and model registry:

```bash
mlflow ui
```

You can compare runs and see the model promoted to "Production" in the "Models" tab.

### 5. Generating the Report

A professional technical report is generated automatically at the end of the pipeline:

```bash
python src/generate_report.py
```

This produces `MLOps_Titanic_Report.pdf`.

### 6. CI/CD

On every push to the `master` branch, GitHub Actions will:

- Check out the code.
- Install dependencies.
- Setup a temporary DVC local remote.
- Run `dvc repro` to verify the pipeline.
- Log metrics (visible in Actions logs).

## Results

- **Best Model**: Random Forest.
- **Top F1-Score**: ~0.80.
- **Production Model**: Registered as `TitanicBestModel` in MLflow.
